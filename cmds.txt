
@Controller
@RequestMapping(value="/getList", method=RequestMethod.GET,produces{"application/json","application/xml"})
@ResponseBody 

controller - services - EmployeeService - EmployeeServiceImpl - dao - daoimpl(jdbctemplate) - 

beeline -u "jdbc:hive2://.net/10000/dataops;principal=hive/_HOST@NAEAST.AD.JPMORGANCHASE.COM?mapred.job.queue.name=DataManagement" -hivevar arg1=ajskdjla -f ".sql";

sentry-used to enforce role based authorization to data and metadata stored on hadoop cluster

Hive
----
HCatalog provide access to hive metastore to other tools on hadoop so they can read n write data to hive's datawarehouse
If Managed table is dropped, data also gets dropped - so create external table and from their load data to managed table
skip header rows from a table in hive - TBLPROPERTIES("skip.header.line.count"="2")
UDF
	write a java class - create jar file - add jar to HIVE CLI classpath - create temporary function in hive pointing to java class - use the same in hiveql
	org.apache.hadoop.hive.ql.exec.UDF;  CREATE TEMPORARY FUNCTION STRIP AS 'org.hardik.letsdobigdata.Strip'; - select strip('  hiveudf') from dummy;
custom serde


Issues/Lessons Learnt
---------------------
Used ORC format but not supported by Impala - later stick to Parquet
Create tables in Hive and Query it through Impala(as it is in-memory, response time will be faster)
Whenever source system changes often, better to use AVRO format as it supports schema evolution
Store data in HBase and access them through Phoenix

POC's
-----
Global ID's
NextPathway-data moved-sql conversion being taken care by us




create external table db_dzwsale_wholesale.tstavro
(id int,
name string)
row format delimited
fields terminated by ','
stored as avro
location '/tenants/opsdata/shared/wholesale/data'
tblproperties('avro.schema.url'='hdfs:///tenants/opsdata/shared/wholesale/schemas/tst.avsc');

